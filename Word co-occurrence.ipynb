{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "reader = csv.reader(open('H:\\Fall 16\\DIC-CSE587\\Labs\\Spark\\input\\Activity2/new_lemmatizer.csv'))\n",
    "\n",
    "# reads the lemmatizer.csv and maps the word and corresponding lemmas into a dictionary.\n",
    "\n",
    "lemmadict = {}\n",
    "for row in reader:\n",
    "    #row=split(\",\", row)\n",
    "    key = row[0]\n",
    "    if key in lemmadict:\n",
    "        pass\n",
    "    lemmadict[key] = ' '.join(row[1:]).split()#filter(None,row[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc=SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reads all the input tess files present in the input directory\n",
    "\n",
    "raw_rdd = sc.textFile('H:\\Fall 16\\DIC-CSE587\\Labs\\Spark\\input\\Activity2\\*.tess')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-grams ( n =2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#mapper function for 2grams\n",
    "\n",
    "def sparkmapper2gram(line):\n",
    "    \n",
    "    results=dict()\n",
    "    #print(line)\n",
    "    fields = line.split(\">\")\n",
    "    #print(fields[0])\n",
    "    location=fields[0]+\">\"\n",
    "    #print(location)\n",
    "    tokens=re.sub('[^a-zA-Z0-9\\\\s]',\"\",(fields[1]))\n",
    "    tokens=re.sub('j','i',tokens)\n",
    "    tokens=re.sub('v','u',tokens)\n",
    "    \n",
    "    tokens=tokens.strip().split(\" \")\n",
    "    \n",
    "    #print(tokens[1])\n",
    "    for i in range(0,(len(tokens)-2)):\n",
    "        firstlist=list()\n",
    "        first=tokens[i].lower()\n",
    "    \n",
    "        if first in lemmadict:\n",
    "            firstlist=lemmadict.get(first)\n",
    "        else:\n",
    "            firstlist=first\n",
    "        for j in range(i+1, len(tokens)-1):\n",
    "            \n",
    "    #print(firstlist)\n",
    "    \n",
    "            second=tokens[j].lower()\n",
    "            secondlist=list()\n",
    "            if second in lemmadict:\n",
    "                secondlist=lemmadict.get(second)\n",
    "            else:\n",
    "                secondlist=second\n",
    "            \n",
    "            for x in firstlist:\n",
    "                for y in secondlist:\n",
    "                    results[x,y]=location\n",
    "\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.170800000000001 seconds\n"
     ]
    }
   ],
   "source": [
    "# Maps all the documents line by line then reduces the values for the same key by appending them(locations)\n",
    "\n",
    "\n",
    "start_time = time.clock()\n",
    "rows=sc.parallelize(raw_rdd.take(raw_rdd.count())).map(sparkmapper2gram).flatMap(lambda x: x.items())\n",
    "#rows=sc.flatMap(lambda x: x.items())\n",
    "cooccured=rows.reduceByKey(lambda pair,locate:pair+locate)\n",
    "print(time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-grams( n=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparkmapper3gram(line):\n",
    "    #print(\"im here\")\n",
    "    results=dict()\n",
    "    #print(line)\n",
    "    fields = line.split(\">\")\n",
    "    #print(fields[0])\n",
    "    location=fields[0]+\">\"\n",
    "    #print(location)\n",
    "    tokens=re.sub('[^a-zA-Z0-9\\\\s]',\"\",(fields[1]))\n",
    "    tokens=re.sub('j','i',tokens)\n",
    "    tokens=re.sub('v','u',tokens)\n",
    "    \n",
    "    tokens=tokens.strip().split(\" \")\n",
    "    \n",
    "    #print(tokens[1])\n",
    "    for i in range(0,(len(tokens)-3)):\n",
    "        firstlist=list()\n",
    "        first=tokens[i].lower()\n",
    "    \n",
    "        if first in lemmadict:\n",
    "            firstlist=lemmadict.get(first)\n",
    "        else:\n",
    "            firstlist=first\n",
    "    #print(firstlist)\n",
    "        for j in range(i+1,len(tokens)-2):\n",
    "            \n",
    "            second=tokens[j].lower()\n",
    "            secondlist=list()\n",
    "            if second in lemmadict:\n",
    "                secondlist=lemmadict.get(second)\n",
    "            else:\n",
    "                secondlist=second\n",
    "            for k in range(j+1,len(tokens)-1):\n",
    "                \n",
    "                third=tokens[k].lower()\n",
    "                thirdlist=list()\n",
    "                if third in lemmadict:\n",
    "                    thirdlist=lemmadict.get(third)\n",
    "                else:\n",
    "                    thirdlist=third\n",
    "            \n",
    "                for x in firstlist:\n",
    "                    for y in secondlist:\n",
    "                        for z in thirdlist:\n",
    "                            results[x,y,z]=location\n",
    "                \n",
    "\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.806895000000001 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "rows3grams=sc.parallelize(raw_rdd.take(raw_rdd.count())).map(sparkmapper3gram).flatMap(lambda x: x.items())\n",
    "#rows3grams=sc.parallelize(rows3grams)\n",
    "cooccured3grams=rows3grams.reduceByKey(lambda pair,locate:pair+locate)\n",
    "print(time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write and display output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cooccured.saveAsTextFile(\"2gramFinalOutput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cooccured3grams.saveAsTextFile(\"3gramFinalOutput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfWithSchema2grams = spark.createDataFrame(cooccured).toDF(\"n=2\", \"location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfWithSchema3grams = spark.createDataFrame(cooccured3grams).toDF(\"n=3\", \"location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|               n=2|            location|\n",
      "+------------------+--------------------+\n",
      "|      [bellum,per]|<luc. 1.1><luc. 1...|\n",
      "|      [bellus,per]|<luc. 1.1><luc. 1...|\n",
      "| [emathius,multus]|          <luc. 1.1>|\n",
      "|    [emathius,qui]|          <luc. 1.1>|\n",
      "|      [multus,qui]|<luc. 1.1><luc. 1...|\n",
      "|    [quis,ciuilis]|<luc. 1.1><luc. 1...|\n",
      "|        [ius,data]|          <luc. 1.2>|\n",
      "|       [ius,datus]|          <luc. 1.2>|\n",
      "|   [scelerus,cano]|          <luc. 1.2>|\n",
      "|     [scelus,cano]|          <luc. 1.2>|\n",
      "|[scelerus,populus]|          <luc. 1.2>|\n",
      "|  [scelus,populus]|          <luc. 1.2>|\n",
      "|    [cano,populus]|          <luc. 1.2>|\n",
      "|     [in,conuerto]|          <luc. 1.3>|\n",
      "|    [suus,uictrix]|          <luc. 1.3>|\n",
      "|    [suus,uiscera]|          <luc. 1.3>|\n",
      "| [uictrix,uiscera]|          <luc. 1.3>|\n",
      "|  [cognatus,rumpo]|          <luc. 1.4>|\n",
      "|  [cognata,foedus]|          <luc. 1.4>|\n",
      "|     [acies,foedo]|          <luc. 1.4>|\n",
      "+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+----------+\n",
      "|                 n=3|  location|\n",
      "+--------------------+----------+\n",
      "|[bellum,emathius,...|<luc. 1.1>|\n",
      "|[bellus,emathius,...|<luc. 1.1>|\n",
      "|[bellum,emathius,...|<luc. 1.1>|\n",
      "|[bellus,emathius,...|<luc. 1.1>|\n",
      "| [bellum,multus,qui]|<luc. 1.1>|\n",
      "| [bellus,multus,qui]|<luc. 1.1>|\n",
      "|[bellum,quis,ciui...|<luc. 1.1>|\n",
      "|[bellus,quis,ciui...|<luc. 1.1>|\n",
      "|[per,emathius,mul...|<luc. 1.1>|\n",
      "|  [per,emathius,qui]|<luc. 1.1>|\n",
      "|    [per,multus,qui]|<luc. 1.1>|\n",
      "|  [per,quis,ciuilis]|<luc. 1.1>|\n",
      "|[emathius,quam,ci...|<luc. 1.1>|\n",
      "|[multus,quam,ciui...|<luc. 1.1>|\n",
      "| [ius,scelerus,cano]|<luc. 1.2>|\n",
      "|   [ius,scelus,cano]|<luc. 1.2>|\n",
      "|[ius,scelerus,pop...|<luc. 1.2>|\n",
      "|[ius,scelus,populus]|<luc. 1.2>|\n",
      "|  [ius,cano,populus]|<luc. 1.2>|\n",
      "|[data,scelerus,cano]|<luc. 1.2>|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithSchema2grams.show()\n",
    "dfWithSchema3grams.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
